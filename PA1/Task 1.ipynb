{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"a8d4606f9d31489fb4efc6b8448c0b4b","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"ae1eda25955d46529fa6bcf0294f02fb","deepnote_cell_type":"text-cell-h1","formattedRanges":[],"is_collapsed":false},"source":["# Task 1: A classification example: fetal heart condition diagnosis"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"07b2e6b69ff84af288009a0da4926c7f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3366,"execution_start":1679851519842,"source_hash":"c2cd4c38"},"outputs":[],"source":["#Imports\n","import pandas as pd\n","from pandas import Series, DataFrame\n","from sklearn.model_selection import train_test_split\n","from sklearn.dummy import DummyClassifier\n","from sklearn.model_selection import cross_val_score\n","import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import metrics\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.linear_model import Perceptron\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n"]},{"cell_type":"markdown","metadata":{"cell_id":"234874d74db2408686a73ecf4ec01b66","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false},"source":["## Step 1. Reading the data"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"73e9c84d136a450bb1bc19fe2e7c1031","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":94,"execution_start":1679851523223,"source_hash":"7076e411"},"outputs":[],"source":["data= pd.read_csv('CTG.csv', skiprows=1)\n","column_names = list((data.columns))\n","selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n","                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n","                 'Median', 'Variance', 'Tendency', 'NSP']\n","data_selected = data[selected_cols].dropna()  \n","data_selected.sample(frac = 1.0, random_state=0)\n","X = data_selected.drop('NSP', axis=1)\n","def to_label(y):\n","    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n","y = data_selected['NSP'].apply(to_label)\n","X_train, X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"5d7f2bd0dd7c48a9a563f0f02f15ff8e","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["20% - 80% splitting technique was used."]},{"cell_type":"markdown","metadata":{"cell_id":"d826255fdc3d40c29aefc340300a70f5","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false},"source":["## Step 2. Training the baseline classifier"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"489d0297827a495b8e6d567a74c23ef5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":37,"execution_start":1679851523335,"source_hash":"ff6e21cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7817647058823529\n","[0.78235294 0.78235294 0.78235294 0.78235294 0.77941176]\n"]}],"source":["#Cross validation of baseline tree classifier\n","clf_freq = DummyClassifier(strategy = 'most_frequent')\n","cross_valid_values = cross_val_score(clf_freq, X_train, y_train)\n","print(np.mean(cross_valid_values))\n","print(cross_valid_values)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"eb5d99b219f2458e9ad5154cdefe0fd0","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["We implemented cross-validation on the Dummy Classifier model. cross_val_score() function calculates the accuracy of the training data with 5 different splits. The average of these accuracies is given above."]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"0b9614f208f94c3aacb298e12c76c427","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":33,"execution_start":1679851523391,"source_hash":"4044f825"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7652582159624414\n","0.7817647058823529\n"]}],"source":["#DummyClassifier\n","clf_freq.fit(X_train,y_train)\n","Yguess = clf_freq.predict(X_test)\n","clf_freq.score(X_test,y_test)  #I am not sure this gives the accuray. \n","print(accuracy_score(y_test, Yguess)) # Accuracy of Test\n","#Accuracy of the train set is 76% with most_frequent strategy\n","\n","y_train_predict = clf_freq.predict(X_train)\n","clf_freq.score(X_train,y_train) \n","print(accuracy_score(y_train, y_train_predict)) # Accuracy of Test\n"]},{"cell_type":"markdown","metadata":{"cell_id":"a4511762cdba4cbaa2d80bf3a23a8a80","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["The accuracy of the train set is 78% with the most_frequent strategy. We split the prediction for the Train and test because we want to measure the accuracy of test dataset and training dataset"]},{"cell_type":"markdown","metadata":{"cell_id":"1ab444a641ad45a9b2ce63a90687a233","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false},"source":["## Step 3. Trying out some different classifiers"]},{"cell_type":"markdown","metadata":{"cell_id":"bb1efb5db4b34343a42fec2c9f253cb9","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false},"source":["## Tree-based classifiers:"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"e2c958966abf41be85711949c1efb2ef","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":356,"execution_start":1679851523428,"source_hash":"a51f7bc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9352941176470588\n","[0.92941176 0.94117647 0.90882353 0.95588235 0.94117647]\n"]}],"source":["#Cross validation of Decision tree classifier\n","clf_tree= DecisionTreeClassifier(criterion= 'gini',splitter='best',max_depth=None,min_samples_split=2,random_state=0)\n","decision_tree_cross_val = cross_val_score(clf_tree, X_train, y_train)\n","print(np.mean(decision_tree_cross_val))\n","print(decision_tree_cross_val)"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"5d704ecb27ba41c3a1be01ccbdd784a9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5407,"execution_start":1679851523792,"source_hash":"7ce4b271"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9476470588235294\n","[0.95294118 0.95588235 0.92941176 0.95882353 0.94117647]\n"]}],"source":["#Cross validation of random forest classifier\n","\n","randomforest_model = RandomForestClassifier()\n","random_forest_cross_val = cross_val_score(randomforest_model, X_train, y_train)\n","print(np.mean(random_forest_cross_val))\n","print(random_forest_cross_val)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"16ce05561dd04466bc67e6d0c9cb8543","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":13899,"execution_start":1679851529210,"source_hash":"bff7db7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.951764705882353\n","[0.96176471 0.95588235 0.94705882 0.95       0.94411765]\n"]}],"source":["grad_model = GradientBoostingClassifier(n_estimators = 100 , learning_rate = 0.1 , max_depth = 2 ,random_state = 0 )\n","grad_model_cross_val = cross_val_score(grad_model, X_train, y_train)\n","print(np.mean(grad_model_cross_val))\n","print(grad_model_cross_val)"]},{"cell_type":"markdown","metadata":{"cell_id":"643062238a6e424381551a0690da643f","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["The tree-based classifiers are types of classification models that predict the categorial class based on the decision trees. The decision tree classifier works by splitting the data into samples to predict the values. On the other hand, the Random forest and Gradient boosting classifiers are the ensemble method that forms multiple decision trees to improve the accuracy. In the Random Forest classifier, the model is trained based on the prediction of each decision tree. Gradient boosting classifier works by training the model at each level of the decision tree by correcting the errors of the previous tree in the next level to improve the accuracy of the model."]},{"cell_type":"markdown","metadata":{"cell_id":"faccef7c90104546926d7376463f697a","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false},"source":["## Linear classifiers:"]},{"cell_type":"markdown","metadata":{"cell_id":"405712f6-d6e2-4ee3-a177-ddc3cfbc1061","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["Perceptron makes predictions based on assigning weights to the input to categorize the output. Linear SVC works by find the boundary by maximizing the margin to divide the classes to improve the performance."]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"a8cae62e34f04dc9888036baaaa5c43a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":368,"execution_start":1679851543125,"source_hash":"c8e0288"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.85294118 0.86176471 0.83823529 0.83235294 0.75588235]\n","0.8282352941176472\n"]}],"source":["#Perceptron cross validation \n","percep_clf = Perceptron(random_state=0)\n","print(cross_val_score(percep_clf,X_train, y_train))\n","print(np.mean(cross_val_score(percep_clf,X_train, y_train)))"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"bec508e1f22d408fa216a1828682efbb","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1079,"execution_start":1679851543518,"source_hash":"51eeb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.85230930682132\n","[0.842723   0.87058824 0.92       0.89882353 0.72941176]\n"]}],"source":["#Logistic Regression model creating and cross validation\n","s_scalar = StandardScaler()\n","X_val = X.copy()\n","X_var1 = s_scalar.fit_transform(X_val)\n","X_train = s_scalar.fit_transform(X_train)\n","X_test = s_scalar.fit_transform(X_test)\n","lr_model = LogisticRegression( max_iter = 10000)\n","lr_cross_val=cross_val_score(lr_model,X_var1,y,cv= 5)\n","print(np.mean(lr_cross_val))\n","print(lr_cross_val)"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"a2c165d7414440688fd0500a2257262e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10834,"execution_start":1679851544603,"source_hash":"adb8f36c"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8471361502347419\n","[0.83568075 0.87058824 0.91529412 0.90352941 0.71058824]\n"]}],"source":["#we also need to do the same thing for the Linear SVC as you will do in logistic regression\n","s_scalar = StandardScaler()\n","X_val = X.copy()\n","svc_clf = LinearSVC(C=1.0, max_iter = 10000)\n","X_var1 = s_scalar.fit_transform(X_val)\n","X_train = s_scalar.fit_transform(X_train)\n","X_test = s_scalar.fit_transform(X_test)\n","svc_cross_val =cross_val_score(svc_clf,X_var1,y,cv= 5)\n","print(np.mean(svc_cross_val))\n","print(svc_cross_val)"]},{"cell_type":"markdown","metadata":{"cell_id":"3af5a56dc3694cd9b1ac180fe619fda4","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false},"source":["## Neural network classifier:"]},{"cell_type":"markdown","metadata":{"cell_id":"330392cb7af747a89968827b2529682a","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["The neural network works by inputting the data in the input layer and passing the data to the hidden layers and the output layer provides the probability distribution of the classes."]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"0547a84b5d2e4032bdc2be0fd6cfb2f4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":62335,"execution_start":1679851555438,"source_hash":"e7abfce2"},"outputs":[],"source":["# Neural networks\n","neural_clf = MLPClassifier(max_iter = 3000)\n","X_value = s_scalar.fit_transform(X_val)\n","X_train = s_scalar.fit_transform(X_train)\n","X_test = s_scalar.fit_transform(X_test)\n","nn_cross_val =cross_val_score(neural_clf,X_value,y,cv= 5)"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"ea43892aeea14862912df43c6ac4f0d4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":19,"execution_start":1679851617773,"source_hash":"29583faf"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8344324772162386\n","[0.83098592 0.88470588 0.90117647 0.87764706 0.67764706]\n"]}],"source":["print(np.mean(nn_cross_val))\n","print(nn_cross_val)"]},{"cell_type":"markdown","metadata":{"cell_id":"c679b9c5a7014c34bf6889df1dc5492a","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false},"source":["## Step 4 Final evaluation"]},{"cell_type":"markdown","metadata":{"cell_id":"ed480033447b4088a45c98b49a196c90","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["The general performance of tree-based classifiers is better than others. Maybe we can get better performance by tuning the hyperparameters from the MLP classifier but with the default parameters, as we observed above, the highest accuracy of cross-validation is the gradient boosting classifier model."]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"c1fd2fbe58c046dba15e8470995d5804","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3909,"execution_start":1679851617793,"source_hash":"ca2f720d"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9436619718309859\n"]}],"source":["grad_model = GradientBoostingClassifier(n_estimators = 100 , learning_rate = 0.1 , max_depth = 2 ,random_state = 0 ).fit(X_train,y_train)\n","grad_model.predict(X_test)\n","print(grad_model.score(X_test,y_test))"]},{"cell_type":"markdown","metadata":{"cell_id":"e9fb090b91434bc1abea0309cd1651c8","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["After fitting the model, the accuracy of the test dataset is 94%. "]},{"cell_type":"markdown","metadata":{"cell_id":"c5cdf240-5251-49a9-a672-9057469a60d2","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false},"source":["Gradient Boosting Classifier is a boosting algorithm that involves building decision trees in iterations, where the predicted values of each subsequent tree aim to reduce the loss function of the previous tree, with its own set of predicted values. The model's overall prediction accuracy is achieved by combining the predictions of all trees at each level of iteration."]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=82bb48ff-b539-4abb-b8e9-17b011c6b53b' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_full_width":false,"deepnote_notebook_id":"391980f18f354ca38c1d7fc788c2a7e0","deepnote_persisted_session":{"createdAt":"2023-03-25T23:00:09.814Z"},"language_info":{"name":"python"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
