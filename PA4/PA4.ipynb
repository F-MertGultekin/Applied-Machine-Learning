{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"e1d642cdf7c0446b8c3fbe3839a4ed40","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Applied Machine Learning PA4"]},{"cell_type":"markdown","metadata":{"cell_id":"6998ee56-7186-4d6c-a7aa-1db36fcc5e62","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"d1e1b946c82a469a86bb6b44e628efd9","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Exercise Question"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"0919f79d01aa447484cea69db1bacee0","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":535,"execution_start":1683312533321,"source_hash":"dbc39fa7"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import Normalizer\n","from sklearn.pipeline import make_pipeline\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.base import BaseEstimator\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","import time"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"4c78506f0bc44870b89a35d28455c1e8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1683312533861,"source_hash":"457d0673"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","0.5\n"]}],"source":["from sklearn.feature_extraction import DictVectorizer\n","from sklearn.linear_model import Perceptron\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.pipeline import make_pipeline\n","\n","X1 = [{'city':'Gothenburg', 'month':'July'},\n","      {'city':'Gothenburg', 'month':'December'},\n","      {'city':'Paris', 'month':'July'},\n","      {'city':'Paris', 'month':'December'}]\n","Y1 = ['rain', 'rain', 'sun', 'rain']\n","\n","X2 = [{'city':'Sydney', 'month':'July'},\n","      {'city':'Sydney', 'month':'December'},\n","      {'city':'Paris', 'month':'July'},\n","      {'city':'Paris', 'month':'December'}]\n","Y2 = ['rain', 'sun', 'sun', 'rain']\n","\n","classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n","classifier1.fit(X1, Y1)\n","guesses1 = classifier1.predict(X1)\n","print(accuracy_score(Y1, guesses1))\n","\n","classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n","#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())\n","classifier2.fit(X2, Y2)\n","guesses2 = classifier2.predict(X2)\n","print(accuracy_score(Y2, guesses2))"]},{"cell_type":"markdown","metadata":{"cell_id":"f012bc7494a449b5959b06627065cad0","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["reasons: "]},{"cell_type":"markdown","metadata":{"cell_id":"10c86f06-7df0-4f9a-bed8-771a588ea7d9","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Very Small dataset"]},{"cell_type":"markdown","metadata":{"cell_id":"55bea302-e0b6-470a-9bf6-55b053f32223","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Overfitting "]},{"cell_type":"markdown","metadata":{"cell_id":"d0cd8958-e8b5-4a1d-98a7-30b4130342d2","deepnote_cell_type":"text-cell-bullet","formattedRanges":[]},"source":["- Generalization problem"]},{"cell_type":"markdown","metadata":{"cell_id":"930a4bfadae64ae994fa0b02ba45a18d","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["There are some problems which cause dramatic decrease when the training set changes. All these problems are related to each other. First of all, the training sets are quite small. Training and learning from the small dataset make the model not generalize well on new data. For example, the original dataset has only two different cities and the second dataset has a new city, Sydney and the model struggles to predict this city's classification. Memorizing the training data means over fitting occurs. Instead of learning the underlying patterns in the dataset, due to lack of data, the model memorizes the dataset. "]},{"cell_type":"markdown","metadata":{"cell_id":"2e5e122b-bdd9-4118-8a91-9e1859662f8f","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"f6f2d5a7879a407e8cca93939d7157db","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Introduction"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"01e079d14d374c17b07d82e4ece5f51f","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1683312533879,"source_hash":"373df801"},"outputs":[],"source":["\n","\"\"\"This file shows a couple of implementations of the perceptron learning\n","algorithm. It is based on the code from Lecture 3, but using the slightly\n","more compact perceptron formulation that we saw in Lecture 6.\n","\n","There are two versions: Perceptron, which uses normal NumPy vectors and\n","matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n","The latter may be faster when we have high-dimensional feature representations\n","with a lot of zeros, such as when we are using a \"bag of words\" representation\n","of documents.\n","\"\"\"\n","\n","import numpy as np\n","from sklearn.base import BaseEstimator\n","\n","class LinearClassifier(BaseEstimator):\n","    \"\"\"\n","    General class for binary linear classifiers. Implements the predict\n","    function, which is the same for all binary linear classifiers. There are\n","    also two utility functions.\n","    \"\"\"\n","\n","    def decision_function(self, X):\n","        \"\"\"\n","        Computes the decision function for the inputs X. The inputs are assumed to be\n","        stored in a matrix, where each row contains the features for one\n","        instance.\n","        \"\"\"\n","        return X.dot(self.w)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts the outputs for the inputs X. The inputs are assumed to be\n","        stored in a matrix, where each row contains the features for one\n","        instance.\n","        \"\"\"\n","\n","        # First compute the output scores\n","        scores = self.decision_function(X)\n","\n","        # Select the positive or negative class label, depending on whether\n","        # the score was positive or negative.\n","        out = np.select([scores >= 0.0, scores < 0.0],\n","                        [self.positive_class,\n","                         self.negative_class])\n","        return out\n","\n","    def find_classes(self, Y):\n","        \"\"\"\n","        Finds the set of output classes in the output part Y of the training set.\n","        If there are exactly two classes, one of them is associated to positive\n","        classifier scores, the other one to negative scores. If the number of\n","        classes is not 2, an error is raised.\n","        \"\"\"\n","        classes = sorted(set(Y))\n","        if len(classes) != 2:\n","            raise Exception(\"this does not seem to be a 2-class problem\")\n","        self.positive_class = classes[1]\n","        self.negative_class = classes[0]\n","\n","    def encode_outputs(self, Y):\n","        \"\"\"\n","        A helper function that converts all outputs to +1 or -1.\n","        \"\"\"\n","        return np.array([1 if y == self.positive_class else -1 for y in Y])\n","\n","\n","class Perceptron(LinearClassifier):\n","    \"\"\"\n","    A straightforward implementation of the perceptron learning algorithm.\n","    \"\"\"\n","\n","    def __init__(self, n_iter=20):\n","        \"\"\"\n","        The constructor can optionally take a parameter n_iter specifying how\n","        many times we want to iterate through the training set.\n","        \"\"\"\n","        self.n_iter = n_iter\n","\n","    def fit(self, X, Y):\n","        \"\"\"\n","        Train a linear classifier using the perceptron learning algorithm.\n","        \"\"\"\n","\n","        # First determine which output class will be associated with positive\n","        # and negative scores, respectively.\n","        self.find_classes(Y)\n","\n","        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n","        Ye = self.encode_outputs(Y)\n","\n","        # If necessary, convert the sparse matrix returned by a vectorizer\n","        # into a normal NumPy matrix.\n","        if not isinstance(X, np.ndarray):\n","            X = X.toarray()\n","\n","        # Initialize the weight vector to all zeros.\n","        n_features = X.shape[1]\n","        self.w = np.zeros(n_features)\n","\n","        # Perceptron algorithm:\n","        for i in range(self.n_iter):\n","            for x, y in zip(X, Ye):\n","\n","                # Compute the output score for this instance.\n","                score = x.dot(self.w)\n","\n","                # If there was an error, update the weights.\n","                if y*score <= 0:\n","                    self.w += y*x\n","\n","\n","##### The following part is for the optional task.\n","\n","### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n","### Here are two utility functions that help us carry out some vector\n","### operations that we'll need.\n","\n","def add_sparse_to_dense(x, w, factor):\n","    \"\"\"\n","    Adds a sparse vector x, scaled by some factor, to a dense vector.\n","    This can be seen as the equivalent of w += factor * x when x is a dense\n","    vector.\n","    \"\"\"\n","    w[x.indices] += factor * x.data\n","\n","def sparse_dense_dot(x, w):\n","    \"\"\"\n","    Computes the dot product between a sparse vector x and a dense vector w.\n","    \"\"\"\n","    return np.dot(w[x.indices], x.data)\n","\n","\n","class SparsePerceptron(LinearClassifier):\n","    \"\"\"\n","    A straightforward implementation of the perceptron learning algorithm,\n","    assuming that the input feature matrix X is sparse.\n","    \"\"\"\n","\n","    def __init__(self, n_iter=20):\n","        \"\"\"\n","        The constructor can optionally take a parameter n_iter specifying how\n","        many times we want to iterate through the training set.\n","        \"\"\"\n","        self.n_iter = n_iter\n","\n","    def fit(self, X, Y):\n","        \"\"\"\n","        Train a linear classifier using the perceptron learning algorithm.\n","\n","        Note that this will only work if X is a sparse matrix, such as the\n","        output of a scikit-learn vectorizer.\n","        \"\"\"\n","        self.find_classes(Y)\n","\n","        # First determine which output class will be associated with positive\n","        # and negative scores, respectively.\n","        Ye = self.encode_outputs(Y)\n","\n","        # Initialize the weight vector to all zeros.\n","        self.w = np.zeros(X.shape[1])\n","\n","        # Iteration through sparse matrices can be a bit slow, so we first\n","        # prepare this list to speed up iteration.\n","        XY = list(zip(X, Ye))\n","\n","        for i in range(self.n_iter):\n","            for x, y in XY:\n","\n","                # Compute the output score for this instance.\n","                # (This corresponds to score = x.dot(self.w) above.)\n","                score = sparse_dense_dot(x, self.w)\n","\n","                # If there was an error, update the weights.\n","                if y*score <= 0:\n","                    # (This corresponds to self.w += y*x above.)\n","                    add_sparse_to_dense(x, self.w, y)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"19750df4591146db8688e9c7aa398548","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1441,"execution_start":1683312533886,"source_hash":"1d6a31c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training time: 1.19 sec.\n","Accuracy: 0.7919.\n"]}],"source":["\n","import time\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import Normalizer\n","from sklearn.pipeline import make_pipeline\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","# This function reads the corpus, returns a list of documents, and a list\n","# of their corresponding polarity labels. \n","def read_data(corpus_file):\n","    X = []\n","    Y = []\n","    with open(corpus_file, encoding='utf-8') as f:\n","        for line in f:\n","            _, y, _, x = line.split(maxsplit=3)\n","            X.append(x.strip())\n","            Y.append(y)\n","    return X, Y\n","\n","\n","if __name__ == '__main__':\n","    \n","    # Read all the documents.\n","    X, Y = read_data('all_sentiment_shuffled.txt')\n","    \n","    # Split into training and test parts.\n","    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n","                                                    random_state=0)\n","\n","    # Set up the preprocessing steps and the classifier.\n","    pipeline = make_pipeline(\n","        TfidfVectorizer(),\n","        SelectKBest(k=1000),\n","        Normalizer(),\n","\n","        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n","        Perceptron()  \n","    )\n","\n","    # Train the classifier.     \n","    t0 = time.time()\n","    pipeline.fit(Xtrain, Ytrain)\n","    t1 = time.time()\n","    print('Training time: {:.2f} sec.'.format(t1-t0))\n","\n","    # Evaluate on the test set.\n","    Yguess = pipeline.predict(Xtest)\n","    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"f0050a85-531f-4b49-b631-3803e40fff85","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"518b97f3f86b45988922da43ec0ba7f7","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Implementing the SVC"]},{"cell_type":"markdown","metadata":{"cell_id":"dfdde450-e19d-4a84-b855-fc0fa7f1961a","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"a92b28c031cf46cf8b7c8317ecb0599e","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The Pegasos (Primal Estimated sub-Gradient Solver) algorithm  is a sub -gradient descent algorithm for optimizing the objective function. The objective is to minimize the loss and maximize the margin between the decision boundary and the data points. The data points that lie closest to the line are called the support vectors. In the implementation, the python class Supportvm is implemented as the Pegasos algorithm. Initially the weight vector is initialized to all zeros. The algorithm begins by iterating over the samples and chooses a set of samples randomly. The weight is updated based on the values, if the dot product value is less than  1 then the weight is updated using the regularized function and loss function, if the sample is classified correctly then the weight is updated using the regularized objective function. "]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"136993e3291f45be9067e83b23ef677c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1683312535331,"source_hash":"9c45cc17"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.base import BaseEstimator\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","import time"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"8bb1acf15f7e49239cdf3fcd4a9be256","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":12,"execution_start":1683312535334,"source_hash":"15d4bd54"},"outputs":[],"source":["def read_data(corpus_file):\n","    X = []\n","    Y = []\n","    with open(corpus_file, encoding='utf-8') as f:\n","        for line in f:\n","            _, y, _, x = line.split(maxsplit=3)\n","            X.append(x.strip())\n","            Y.append(y)\n","    return X, Y\n"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"8daa17e2fd5540b4bfd4bf227eddadb0","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":26,"execution_start":1683312535349,"source_hash":"89a2336d"},"outputs":[],"source":["if __name__ == '__main__':\n","\n","    X, Y = read_data('all_sentiment_shuffled.txt')\n","    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,random_state=0)"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"415dcae59a2f4c198aad64a39850f9ae","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1683312535420,"source_hash":"e4be7a3"},"outputs":[],"source":["class LinearClassifier(BaseEstimator):\n","    \n","    def decision_function(self, X):\n","        \n","        return X.dot(self.w)\n","\n","    def predict(self, X):\n","        scores = self.decision_function(X)\n","        out = np.select([scores >= 0.0, scores < 0.0],\n","                        [self.positive_class,\n","                         self.negative_class])\n","        return out\n","\n","    def find_classes(self, Y):\n","        \n","        classes = sorted(set(Y))\n","        if len(classes) != 2:\n","            raise Exception(\"this does not seem to be a 2-class problem\")\n","        self.positive_class = classes[1]\n","        self.negative_class = classes[0]    \n","\n","    def encode_outputs(self, Y):\n","        \n","        return np.array([1 if y == self.positive_class else -1 for y in Y])"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"10a6f3ed4d5e4f06ae017c810f3d73a2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1683312535421,"source_hash":"79b227cc"},"outputs":[],"source":["class Supportvc(LinearClassifier):\n","    \"\"\"Implementation of pegasos svc  \"\"\"\n","    def __init__(self, lambda_value =1, n_iter = 25):\n","        #self.step_size = step_size\n","        self.lambda_value = lambda_value\n","        self.n_iter = n_iter\n","        self.w = None\n","    \n","\n","    def fit(self, X, Y):\n","       \n","        self.find_classes(Y)\n","        Ye = self.encode_outputs(Y)\n","        X = X.toarray()\n","        n_features = X.shape[1]\n","        n_samples = X.shape[0]\n","        self.w = np.zeros(n_features)\n","  \n","        for t in range(self.n_iter):\n","            i = np.random.choice(n_samples,1)[0]\n","            x=X[i]\n","            y = Ye[i]\n","            eta = 1 / (self.lambda_value * (t+1))\n","            if y * np.dot(self.w,x ) < 1:\n","                self.w  = (1 - eta*self.lambda_value) * self.w + eta * y * x\n","            else:\n","                self.w  = (1 - eta*self.lambda_value) * self.w  \n","        return  self.w "]},{"cell_type":"markdown","metadata":{"cell_id":"12b3029604df46a28575cc02a132845e","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The pipeline class is used to streamline the process of vectorization, feature selection and model training.\n","The Countvectorizer, SelectKbest  and the model svm_classifier is trained using the pegasos algorithm.\n","The key hyperparameters of the algorithm n_iter and lambda_value is tuned to optimize the model such \n","that the performance and accuracy of the model can be increased.\n","The model has resulted in 0.80 accuracy  show the model predicts 80 % of the data correctly.\n"]},{"cell_type":"code","execution_count":29,"metadata":{"cell_id":"9eea557db8b341faade99e841d892ccb","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":15091,"execution_start":1683315136370,"source_hash":"2ca34b3e"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Time: 14.426332712173462 seconds.\n","accuracy: 0.8166177087704574\n"]}],"source":["svc_classifier = Pipeline( [('vectorizer', CountVectorizer(preprocessor = lambda x: x)),\n","    ('sk', SelectKBest(k=5000)),\n","    ('svm', Supportvc(n_iter=len(Xtrain)*10, lambda_value=0.015))] )\n","t0 = time.time()\n","svc_classifier.fit(Xtrain, Ytrain)\n","t1 = time.time()\n","print(' Time:', t1-t0, 'seconds.')\n","Yvalue = svc_classifier.predict(Xtest)\n","print('accuracy:', accuracy_score(Ytest, Yvalue))\n"]},{"cell_type":"markdown","metadata":{"cell_id":"ad4e0d92dcd241c29f045274091aae7d","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["\n","The Grid search CV is used to find the best hyper parameters for the models ,the parameter defined  are set with a range of hyper parameter values to evaluate the model accuracy .The output of the grid search show the best hyper parameters for the model and accuracy.\n"]},{"cell_type":"code","execution_count":28,"metadata":{"cell_id":"e0cd7d710ff54da6846d4171adaa16d9","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":379198,"execution_start":1683315197174,"source_hash":"13a3dc5f"},"outputs":[],"source":["\n","parameter = { 'sk__k': [1000,5000, 10000], \n","    'svm__lambda_value': [0.1,0.015, 0.02],\n","    'svm__n_iter': [len(Xtrain)*10]}\n","grid_search = GridSearchCV(svc_classifier, parameter, cv=5,n_jobs=-1,  scoring='accuracy')\n","grid_search.fit(Xtrain, Ytrain)\n","print('Best parameters:', grid_search.best_params_)\n","print('Validation accuracy:', grid_search.best_score_)"]},{"cell_type":"markdown","metadata":{"cell_id":"64092496b06a4b8a89e08026660c1984","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Implementing the Logistic Regression"]},{"cell_type":"code","execution_count":18,"metadata":{"cell_id":"c8d45f780159432ca1f3d4ad04b88565","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1683313174881,"source_hash":"6192e6fc"},"outputs":[],"source":["class LRPegasos(LinearClassifier):\n","    \"\"\"Implementation of LR with SGD with PEGASOS Algorithm\"\"\"\n"," \n","    def __init__(self, n_iter=1000, lambda1=0.001):\n","        self.n_iter = n_iter\n","        self.lambda1 = lambda1\n","        self.w = None\n"," \n","    def fit(self, X, Y,):\n","       \n","        self.find_classes(Y)\n","        # convert all output values to +1 or -1\n","        Yn = self.encode_outputs(Y) #As usual, the outputs (in the list Y) are coded as +1 for positive examples and -1 for negative examples.\n","        X = X.toarray()\n","        m, n_features = X.shape[0], X.shape[1]\n","        self.w = np.zeros( n_features )\n","        for t in range(self.n_iter):\n","            eta = 1. / (self.lambda1*(t+1)) #The number (Greek letter eta) is the step length or learning rate in gradient descent\n","            j = np.random.choice(m, 1)[0] # pick a fixed number of randomly selected training pairs.\n","            x, y = X[j], Yn[j]\n","            score = x.dot(self.w) \n","            # Log Loss          \n","            self.w = ((1-eta*self.lambda1)*self.w) + (eta *y/(1+np.exp(y*score))*x)\n","           \n","        return self.w\n"]},{"cell_type":"markdown","metadata":{"cell_id":"d773c851813b44f98525a85b7eac8ce3","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["It is almost the same as the SVC except for updating weights. Here, there is no condition for updating weight. The loss function differs. This is log loss and svc uses hinge loss.  This part \"((1-eta*self.lambda1)*self.w)\" is the same as svc and the other part is the gradient of the log loss"]},{"cell_type":"code","execution_count":26,"metadata":{"cell_id":"88ac760c51c74ccd84d4d1bc8c898a99","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":16396,"execution_start":1683313843084,"source_hash":"fa342e0a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_540/1630960913.py:22: RuntimeWarning: overflow encountered in exp\n","  self.w = ((1-eta*self.lambda1)*self.w) + (eta *y/(1+np.exp(y*score))*x)\n"," Time: 15.775731325149536 seconds.\n","0.8124213176668066\n"]}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","\n","classifier = Pipeline( [('vectorizer', CountVectorizer(preprocessor = lambda x: x)),('fs', SelectKBest(k=5000)),('cls', LRPegasos(n_iter=len(Xtrain)*10, lambda1=0.0015))] )\n","t0 = time.time()\n","classifier.fit(Xtrain, Ytrain)\n","t1 = time.time()\n","print(' Time:', t1-t0, 'seconds.')\n","pred2 = classifier.predict(Xtest)\n","accuracy2 = accuracy_score(Ytest, pred2)\n","print(accuracy2)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"df9d9c97bdf24cdf9812b6036abecef8","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Compared to SVC, LR has almost the same accuracy and training time performance. When we tried with the different vectorizer such as TF-IDF, we get the same results. When we increased the number of iterations len(Xtrain)*100, the accuracy rose slightly by 1% but the training time went up incredibly. "]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c247be64-9f47-45f6-9108-f49f6e841fe2' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"63bec1e00c244421a29eef960e5956b2","deepnote_persisted_session":{"createdAt":"2023-05-05T18:16:57.096Z"},"language_info":{"name":"python"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
